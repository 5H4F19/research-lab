1

Correlation Study Between a Multimodal Teacher
Performance Evaluation System and Traditional
Student Feedback
Shafiqul Islam
Begum Rokeya University,Rangpur

Abstract
Teacher performance evaluation is a cornerstone of educational quality assurance. Traditionally, student feedback has been
the primary tool for assessing teaching effectiveness, but it is often subjective and limited in scope. This proposal outlines a
comparative study between a novel multimodal system—which objectively analyzes classroom interactions—and conventional
student feedback. The study aims to determine the reliability, validity, and impact of both approaches.
Index Terms
Teacher evaluation, multimodal system, student feedback, educational assessment, comparative study

I. I NTRODUCTION
The evaluation of teacher performance is a cornerstone of educational quality assurance and institutional improvement [1].
Effective teaching not only enhances student learning outcomes but also shapes the reputation and development of educational
institutions [2]. Traditionally, the assessment of teaching effectiveness has relied heavily on student feedback, which, while
valuable, is often subject to various biases and limitations [1]. These include subjectivity, inconsistency, and the influence
of non-academic factors such as personal rapport, grading leniency, or classroom environment [3]. Research has shown that
demographic factors, such as gender and race, can also influence student evaluations, raising concerns about fairness and
validity [3].
Despite these drawbacks, student feedback remains prevalent due to its simplicity, cost-effectiveness, and ability to capture
students’ perspectives on teaching quality [2]. However, the reliability of student feedback is challenged by inconsistencies
across different cohorts and courses, as well as by the tendency for students to focus on surface-level attributes rather than
deeper pedagogical competencies [4]. Several studies have highlighted the need for more robust and objective evaluation
mechanisms to complement or replace traditional feedback [5].
Recent advancements in educational technology have paved the way for more objective and comprehensive evaluation methods
[6]. Among these, multimodal systems that leverage data from multiple sources—such as audio, video, gesture recognition,
and classroom interaction analytics—offer promising alternatives [7]. These systems can provide a holistic view of teacher
performance by capturing a wide range of behavioral and communicative cues that are often overlooked in traditional feedback
mechanisms [8].
Artificial intelligence (AI) and machine learning (ML) techniques have enabled the automated analysis of complex classroom
behaviors, including teacher movement, speech patterns, engagement strategies, and student responses [6]. For instance, pose
estimation algorithms can detect and classify teaching activities, while emotion recognition systems can assess the affective
climate of the classroom [8]. These technologies not only enhance the objectivity of evaluations but also provide granular
feedback that can inform targeted professional development [9].
Despite the potential of multimodal systems, there is a lack of empirical studies comparing their effectiveness with conventional student feedback [5]. This research aims to bridge this gap by conducting a comparative study between a multimodal
teacher performance evaluation system and traditional student feedback methods. The primary objectives of this study are to:
• Analyze the strengths and weaknesses of both evaluation approaches [1], [4], [7].
• Assess the reliability and validity of multimodal data in reflecting true teaching effectiveness [6], [10], [9].
• Investigate the correlation between multimodal system verdicts and student perceptions [2], [11], [12].
• Provide recommendations for integrating advanced evaluation systems into existing educational frameworks [5], [7], [8].
The remainder of this paper is organized as follows: Section II reviews related work in teacher evaluation and multimodal
systems. Section III describes the proposed methodology, including data collection and analysis techniques. Section IV presents
the experimental setup, including classroom environment, hardware, and dataset details. Section V details the multimodal system
architecture and implementation, including the system pipeline (see Figure 4) and output classification (see Table V). Section
VI presents the results and discussion, and Section VII concludes the paper with final remarks and future directions.
The integration of multimodal systems into teacher evaluation frameworks represents a significant shift in educational
assessment paradigms. These systems not only address the limitations of traditional feedback but also align with broader trends

2

Teacher Evaluation Methods
Hybrid/Multimodal

Student Feedback-Based
Auditory-Based

Linguistic-Based

Vision-Based
Fig. 1. Taxonomy of teacher performance evaluation methods.

in educational technology and data-driven decision-making. For instance, the use of machine learning algorithms to analyze
multimodal data streams enables the identification of nuanced teaching behaviors that correlate with student engagement and
learning outcomes. Furthermore, the adoption of such systems has practical implications for teacher training and professional
development, as they provide actionable insights that can guide instructional improvement.
However, the implementation of multimodal evaluation systems is not without challenges. Issues such as data privacy,
the need for robust validation across diverse educational contexts, and the potential for algorithmic bias must be carefully
considered. Despite these challenges, the potential benefits of multimodal systems—such as enhanced reliability, validity, and
comprehensiveness—make them a promising complement to traditional student feedback. This study aims to explore these
dynamics by conducting a comparative analysis of both evaluation approaches, thereby contributing to the growing body of
literature on innovative educational assessment methods.
II. R ELATED W ORK
A. Student Feedback-Based Evaluation
Student feedback has long been the predominant method for evaluating teaching effectiveness in higher education. Its
widespread use is attributed to its simplicity, cost-effectiveness, and ability to capture students’ perspectives on instructional
quality [2], [11]. However, research has consistently highlighted significant limitations, including subjectivity, bias, and the
influence of non-academic factors such as instructor popularity, grading leniency, and course difficulty [1], [4]. Demographic
factors, such as gender and race, can also affect student evaluations, raising concerns about fairness and validity [3]. These
issues have prompted calls for more objective and reliable assessment methods [5].
Despite its limitations, student feedback remains a central component in most institutional evaluation frameworks. Many
universities rely on end-of-term surveys or online platforms to collect student opinions, which are then used for faculty
appraisal, promotion, and professional development. However, the overreliance on student feedback can sometimes lead to
unintended consequences, such as grade inflation or a focus on entertainment over educational rigor. Furthermore, the lack
of standardization in survey instruments and interpretation of results can introduce inconsistencies across departments and
institutions. Recent studies have also explored the psychological impact of student evaluations on teachers, noting increased
stress and potential discouragement among faculty who receive negative or biased feedback. These findings underscore the
need for complementary evaluation methods that can provide a more balanced and holistic view of teaching effectiveness.
B. Auditory-Based Evaluation
Auditory-based evaluation methods analyze audio data from classroom interactions to assess teaching performance. Techniques in this domain include speech recognition, prosody analysis, and emotion detection from vocal cues. These approaches
can provide insights into teacher-student engagement, clarity of instruction, and the emotional climate of the classroom.
Machine learning and signal processing advancements have enabled more accurate and automated auditory assessments, though
challenges remain in handling noisy environments and diverse speaking styles [6], [10].
In the context of teacher evaluation, auditory analysis offers a unique perspective by focusing on the nuances of verbal
communication. For example, the tone, pace, and modulation of a teacher’s voice can influence student engagement and
comprehension. Studies have shown that enthusiastic and expressive speech is often associated with higher student motivation
and participation. Additionally, auditory features can be used to detect classroom dynamics, such as the frequency of teacherstudent interactions or the presence of collaborative discussions. Integrating auditory data with other modalities can help address
the limitations of purely subjective feedback, offering a more objective measure of classroom engagement and instructional
quality.
C. Vision-Based Evaluation
Vision-based evaluation leverages video data to objectively assess teacher performance. Methods include gesture recognition,
pose estimation, and analysis of classroom movement patterns. These techniques capture non-verbal communication, instructional delivery, and engagement strategies. Computer vision and deep learning have significantly advanced the capabilities of
vision-based systems, enabling detailed behavioral analysis in real-time classroom settings [7], [8], [13], [14], [15], [9].

3

TABLE I
R ELIABILITY OF T EACHER E VALUATION M ETHODS
Method

Reliability

Student Feedback-Based
Auditory-Based
Vision-Based
Linguistic-Based
Hybrid/Multimodal

Low–Medium
Medium
High
Medium
High

Beyond technical advancements, vision-based evaluation aligns closely with the multimodal theme of this study by providing
a window into the physical and social aspects of teaching. Non-verbal cues, such as gestures, facial expressions, and movement
around the classroom, play a critical role in effective pedagogy. For instance, teachers who frequently interact with students
through eye contact or open body language are often perceived as more approachable and supportive. Vision-based systems
can also identify patterns of classroom management, such as how teachers distribute their attention or facilitate group activities.
By quantifying these behaviors, vision-based evaluation complements traditional feedback and provides actionable insights for
professional development. Moreover, the integration of visual data with auditory and linguistic information can create a richer,
more nuanced understanding of teaching practices.
D. Linguistic and Discourse-Based Evaluation
Linguistic and discourse-based evaluation focuses on analyzing the content, structure, and sentiment of spoken or written
language used by teachers. Natural language processing (NLP) techniques are employed to assess instructional clarity, discourse
structure, sentiment, and the use of pedagogical language [12], [9], [16], [17], [18], [19], [20], [21], [22]. Automated discourse
analysis and sentiment detection have emerged as powerful tools for evaluating communication skills and the ability to convey
complex concepts. Recent studies have explored emotion analysis, topic modeling, and engagement detection in educational
contexts, highlighting the growing role of NLP in teacher evaluation.
E. Hybrid and Multimodal Evaluation Approaches
Hybrid and multimodal evaluation systems integrate data from multiple sources—such as audio, video, and linguistic
features—to provide a comprehensive assessment of teacher performance. These systems aim to overcome the limitations
of single-modality approaches by capturing a broader range of behavioral and communicative cues. Studies have shown that
multimodal systems can enhance the reliability and validity of teacher evaluations, offering more granular and actionable
feedback [5], [7], [8]. However, challenges remain regarding data integration, privacy, and the need for robust validation in
diverse educational contexts.
As summarized in Figure 1, existing teacher evaluation methods can be broadly categorized into five main approaches, each
with distinct strengths and limitations.
III. M ETHODOLOGY
This section outlines the research design, data collection methods, and analytical techniques employed in this comparative
study of teacher evaluation approaches.
A. Research Design and Approach
This study employs a mixed-methods comparative design to evaluate traditional student feedback and multimodal evaluation
systems. The research follows a parallel convergent approach where both evaluation methods are applied simultaneously to the
same teaching instances, allowing for direct comparison while minimizing contextual variations. The study will be conducted in
a real-world classroom setting, focusing on higher education institutions. The multimodal evaluation system will be implemented
in a controlled environment, ensuring that both student feedback and multimodal data are collected under similar conditions.
This design allows for a comprehensive analysis of the strengths and weaknesses of each evaluation method. The research
will utilize a combination of quantitative and qualitative data collection methods, including standardized surveys, audio-visual
recordings, and discourse analysis. The quantitative data will be analyzed using statistical techniques to identify correlations and
patterns, while qualitative data will undergo thematic analysis to extract meaningful insights. The study will also incorporate
a longitudinal component, allowing for the examination of changes in teaching effectiveness over time. By collecting data at
multiple points throughout the semester, the research aims to capture the dynamic nature of teaching and learning processes.
The primary research questions guiding this study are:
1) To what extent do multimodal evaluations correlate with traditional student feedback?
2) Which aspects of teaching effectiveness are captured more accurately by each evaluation method?

4

- Voice prosody
- Pitch variation
- Speech rate
- Pause patterns

- Facial expressions
- Gestures
- Proxemics
- Class movement

- Language complexity
- Question types
- Feedback patterns
- Explanatory quality

Audio Analysis
Module

Visual Analysis
Module

Discourse
Analysis Module

Multimodal
Integration Module

Fig. 2. Multimodal system components and feature extraction modules.

3) How can multimodal systems complement student feedback to provide a more comprehensive evaluation?
4) What are the practical implications of incorporating multimodal evaluations in institutional assessment frameworks?
TABLE II
PARTICIPANT D ISTRIBUTION ACROSS D ISCIPLINES AND E XPERIENCE L EVELS
Discipline
STEM
Humanities
Social Sciences

Novice

Experienced

Expert

4
4
4

4
4
4

2
2
2

B. Participants and Sampling
The study will employ purposive sampling to select 30 instructors from diverse academic disciplines. The inclusion criteria
prioritize representativeness across teaching experience (novice to expert), course level (undergraduate and graduate), and
subject area (STEM, humanities, and social sciences). Each instructor will be evaluated during 3 different teaching sessions,
generating a total of 90 distinct teaching instances for analysis.
Student evaluators will include all enrolled students in the selected course sections, with an estimated total of 1,2001,500 student participants. Demographic information will be collected from both instructors and students to examine potential
correlation patterns and biases.
C. Data Collection Methods
1) Student Feedback Instruments: Traditional evaluation data will be collected using two complementary instruments:
• A standardized quantitative evaluation form using a 5-point Likert scale covering seven dimensions of teaching effectiveness
(clarity, organization, engagement, assessment, feedback, accessibility, and overall effectiveness)
• Open-ended qualitative questions eliciting specific comments on teaching strengths, areas for improvement, and notable
classroom experiences
2) Multimodal System Components: The multimodal evaluation system integrates data from three primary sources, as
illustrated in Figure 2:
1) Audio Module: Captures speech dynamics using directional microphones positioned strategically in the classroom. The
system extracts features related to vocal variety, speech clarity, and emotional tone.
2) Visual Module: Employs two wide-angle cameras (front and rear) to capture teacher movements, gestures, and interactions with students. A deep learning-based pose estimation algorithm tracks key behavioral indicators.
3) Discourse Module: Applies NLP techniques to analyze transcribed classroom dialogue, identifying patterns of instruction,
questioning techniques, and feedback quality.
Data collection will occur simultaneously for both evaluation methods during the same teaching sessions to ensure valid
comparisons.
D. Data Processing and Feature Extraction
1) Audio Data Processing: Audio data will be processed to extract the following features:
• Prosodic features (pitch, intensity, and speech rate)

5

Raw Video Input
Preprocessing

- Frame normalization
- Noise reduction

Human Detection

- YOLO v5
- Person segmentation

Pose Estimation

- OpenPose
- HRNet

Motion Tracking

- Kalman filtering
- Temporal association

Feature Extraction

- Movement patterns
- Interaction zones
- Gesture recognition

Behavior Classification

- SVM classifier
- Random Forest

Fig. 3. Visual data processing pipeline for teacher behavior analysis.
TABLE III
E VALUATION D IMENSIONS AND C ORRESPONDING M ETRICS
Dimension

Student Feedback Metric

Multimodal Metric

Engagement

Likert rating (1-5)

Interaction frequency +
Voice animation index

Clarity

Likert rating (1-5)

Speech rate + Pause ratio +
Example frequency

Organization

Likert rating (1-5)

Topic coherence score +
Transition clarity index

Responsiveness

Likert rating (1-5)

Response time +
Student engagement rate

Voice quality parameters (jitter, shimmer, and harmonic-to-noise ratio)
Temporal features (speaking time, pause duration, and turn-taking patterns)
• Emotion indicators (valence and arousal levels)
Audio processing will employ the PRAAT acoustic analysis software with custom scripts for feature extraction, followed by
normalization to account for individual voice characteristics.
2) Visual Data Analysis: Visual analysis will focus on extracting behavioral indicators using the following pipeline:
The system will quantify spatial classroom dynamics, including:
• Classroom coverage (percentage of classroom space utilized)
• Proximity patterns (time spent in different classroom zones)
• Student interaction frequency (number and distribution of individual engagements)
• Gesture frequency and type (emphatic, illustrative, and regulatory)
3) Linguistic and Discourse Analysis: Classroom dialogue will be transcribed automatically using speech-to-text technology
and analyzed for:
• Question complexity (based on Bloom’s taxonomy)
• Wait time after questions
• Feedback patterns (evaluative, corrective, or elaborative)
• Language complexity (lexical diversity and sentence structure)
• Instructional clarity indicators (use of examples, analogies, and summaries)
4) Student Feedback Processing: Quantitative feedback will be analyzed using descriptive and inferential statistics, while
qualitative comments will undergo thematic analysis using a dual-coding approach to identify emergent patterns. NLP techniques
will also be applied to extract sentiment and topical focus from written comments.
•
•

E. Evaluation Metrics
The comparative analysis will employ the following metrics to assess the relationship between traditional and multimodal
evaluations:
Statistical analyses will include:
• Correlation analysis between student ratings and multimodal metrics

6

Factor analysis to identify underlying constructs across evaluation methods
Multiple regression to predict student satisfaction from multimodal features
• Paired comparisons to identify systematic differences between methods
•
•

F. Ethical Considerations
This research has received approval from the Institutional Review Board (IRB) and implements the following ethical
safeguards:
• Informed consent from all participating instructors and students
• Data anonymization protocols for both traditional and multimodal datasets
• Secure data storage with encryption and access controls
• Options for participants to review their data and withdraw at any time
• Transparent communication about data usage and research findings
All classroom recordings will be processed on secure, local servers rather than cloud-based solutions to enhance privacy
protection. Face-blurring technology will be applied to student images in accordance with privacy regulations.
IV. E XPERIMENTAL S ETUP
This section describes the environment, tools, and procedures used to conduct the comparative study between the multimodal
teacher evaluation system and traditional student feedback.
A. Classroom Environment
Experiments were conducted in real university classrooms across three academic departments (STEM, Humanities, Social
Sciences). Each classroom was equipped with standard teaching facilities and additional sensors for multimodal data collection.
B. Hardware and Software
Audio: Directional microphones (Shure MX391) placed at the front and rear of the classroom.
Video: Two wide-angle HD cameras (Logitech C920) positioned to capture both teacher and student interactions.
• Computing: A dedicated workstation (Intel i7, 32GB RAM, NVIDIA RTX 3060) for real-time data processing and
storage.
• Software:
– PRAAT for audio feature extraction
– OpenPose/HRNet for pose estimation
– Python (NumPy, pandas, scikit-learn) for data analysis
– Custom NLP pipeline for discourse analysis
•
•

C. Data Collection Procedure
1) Session Preparation: Instructors and students were briefed and consent was obtained. Equipment was set up before each
session.
2) Recording: Each teaching session (50 minutes) was recorded for both audio and video. Student feedback was collected
immediately after each session via online forms.
3) Synchronization: All data streams were time-synchronized using a central clock to ensure accurate multimodal analysis.
4) Data Storage: Raw data was securely stored on encrypted local drives. Only anonymized data was used for analysis.
D. Dataset Overview
A total of 90 teaching sessions were recorded (30 instructors × 3 sessions each). For each session, the following data was
collected:
• Audio recordings (WAV, 44.1kHz)
• Video recordings (MP4, 1080p)
• Automatic transcripts (TXT)
• Student feedback responses (CSV)
V. M ULTIMODAL S YSTEM A RCHITECTURE AND I MPLEMENTATION
Overview: The proposed system is a modular, end-to-end multimodal machine learning pipeline that processes audio, video,
and transcript data streams in parallel, fuses their representations, and predicts teaching effectiveness using a unified classifier.
This architecture leverages state-of-the-art models and best practices from the HuggingFace ecosystem and the broader machine
learning community.

7

TABLE IV
S UMMARY OF C OLLECTED DATASET
Data Type

Sessions

Total Size

90
90
90
90

18 hours (12 GB)
18 hours (90 GB)
1.2M words (8 MB)
1,350 responses (0.5 MB)

Audio
Video
Transcripts
Student Feedback

Text Pipeline
BERT Embedding

NLP Feature Extraction

Preprocessing

Preprocessing

YOLOv5 Detection

Feature Extraction

Pose Estimation (HRNet)

Wav2Vec2 Embedding

Audio Pipeline

Video Pipeline

Preprocessing

Motion Tracking

Feature Extraction

Multimodal Fusion Layer

Classifier
(Category)

Fig. 4. High-level architecture of the proposed multimodal teacher evaluation system. Each modality is processed by a dedicated pipeline; their features are
fused with session metadata and classified.

A. Video Stream pipeline
Preprocessing: Video frames are normalized and denoised.
Human Detection: YOLOv5 (via HuggingFace) detects all people in each frame.
• Pose Estimation: HRNet or OpenPose extracts skeletal keypoints for each detected person.
• Motion Tracking: Kalman filtering links poses across frames to track teacher movement.
• Feature Extraction: Computes gesture frequency, classroom coverage, interaction zones, and movement patterns.
•
•

B. Audio Stream pipeline
Preprocessing: Audio is denoised and segmented.
Feature Extraction: PRAAT and Python extract prosodic features (pitch, intensity, speech rate) and emotion embeddings.
• Audio Embedding: Wav2Vec2 (HuggingFace Transformers) produces deep audio representations.
•
•

C. Text Stream pipeline
Preprocessing: Transcripts are cleaned and tokenized.
NLP Feature Extraction: Sentiment analysis, question type detection, and discourse structure are computed.
• Text Embedding: BERT or DistilBERT (HuggingFace Transformers) generates semantic embeddings.
•
•

8

TABLE V
T EACHER E VALUATION O UTPUT C ATEGORIES
Category

Description

Outstanding

Consistently exceeds expectations in all evaluation dimensions.
Frequently exceeds expectations; minor areas
for growth.
Meets expectations in most areas; some
strengths and some areas to improve.
Adequate performance; meets minimum
standards but with clear room for improvement.
Below expectations in several areas; targeted
development required.
Consistently below standards; significant intervention needed.

Very Good
Good
Satisfactory
Needs
Improvement
Unsatisfactory

D. Multimodal Fusion and Classification
Fusion Layer: All modality embeddings/features are concatenated and combined with session metadata (e.g., class size,
subject).
• Classifier: The fused vector is input to a fully connected neural network with a softmax (for categorical) or regression
(for continuous) output, predicting teaching effectiveness.
Modeling and Training:
• All models are implemented in PyTorch, leveraging HuggingFace Transformers for pretrained components.
• Training follows standard ML protocols: stratified train/validation/test splits, cross-entropy or MSE loss, Adam optimizer,
and early stopping.
• Cross-modal alignment is ensured by synchronizing timestamps and using late fusion for interpretability.
• The system is modular and extensible, allowing new modalities or metadata to be added with minimal changes.
Privacy and Ethics: All data is anonymized; student faces are blurred in video, and all processing is performed on secure,
local servers.
This detailed implementation ensures the system is robust, interpretable, and aligned with current machine learning standards
for multimodal educational analytics.
•

E. System Output and Classification
The output of our multimodal teacher evaluation system is a categorical label that summarizes the overall teaching effectiveness for each observed session. This label is generated by the classifier based on fused features from audio, video, and
transcript data, as well as session metadata. The categories are designed to be both interpretable and actionable, providing
clear feedback to educators and administrators without excessive granularity or oversimplification.
The classification is as follows (see Table V):
Each session is assigned one of these categories, which can be used for formative feedback, professional development
planning, or institutional reporting. The system is also capable of providing a confidence score for each prediction, and can
generate a brief textual summary highlighting the key factors influencing the classification (e.g., engagement level, clarity,
responsiveness). This approach ensures that the output is both meaningful and actionable for stakeholders.
VI. R ESULTS AND D ISCUSSION
Present the findings of the comparative study here. Include quantitative and qualitative results, statistical analyses, and a
discussion interpreting the significance of the results in the context of existing literature.
VII. C ONCLUSION AND F UTURE W ORK
Summarize the main contributions and findings of the study. Discuss limitations and propose directions for future research.
ACKNOWLEDGMENT
(Optional) Acknowledge any funding sources, institutional support, or individuals who contributed to the research but are
not listed as authors.

9

R EFERENCES
[1] T. Heffernan, “Sexism, racism, prejudice, and bias: a literature review and synthesis of research surrounding student evaluations of courses and teaching,”
Assessment & Evaluation in Higher Education, vol. 47, no. 1, pp. 144–154, 2022. [Online]. Available: https://doi.org/10.1080/02602938.2021.1888075
[2] M. Ajmal, I. Basit, and S. Sadaf, “Evaluating the role of students’ feedback in enhancing teaching effectiveness,” Pakistan Journal of Humanities and
Social Sciences, vol. 12, 05 2024.
[3] M. P. Steinberg and L. Sartain, “What explains the race gap in teacher performance ratings? evidence from chicago public schools,” Educational
Evaluation and Policy Analysis, vol. 43, no. 1, pp. 60–82, 2021. [Online]. Available: https://doi.org/10.3102/0162373720970204
[4] P. N. Carvalho and M. V. H. Carvalho, “Several biases in evaluation process of professors by undergraduate students,” International Journal for
Innovation Education and Research, vol. 10, no. 7, pp. 433–441, 2022. [Online]. Available: https://doi.org/10.31686/ijier.vol10.iss7.3838
[5] S. Ginsburg and L. Stroud, “Necessary but insufficient and possibly counterproductive: The complex problem of teaching evaluations.” Academic
medicine : journal of the Association of American Medical Colleges, 2022. [Online]. Available: https://api.semanticscholar.org/CorpusID:254961390
[6] Z. Wang, T. Xu, and L. Wang, “Teaching evaluation method based on fuzzy support vector machine algorithm,” Mobile Information Systems, vol. 2022,
07 2022.
[7] Y. Wang, H. Hu, and H. Gao, “Teacher classroom behavior detection based on a human pose estimation algorithm,” in Artificial Intelligence and Robotics,
H. Lu and J. Cai, Eds. Singapore: Springer Nature Singapore, 2024, pp. 68–75.
[8] R. Hou, T. Fütterer, B. Bühler, E. Bozkir, P. Gerjets, U. Trautwein, and E. Kasneci, “Automated assessment of encouragement and warmth in classrooms
leveraging multimodal emotional features and chatgpt,” 04 2024.
[9] Y. Ye, J. Wang, P. He, J. Nie, J. Xiong, and H. Gao, “An action analysis algorithm for teachers based on human pose estimation,” Computers and
Electrical Engineering, vol. 111, p. 108915, 2023. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S0045790623003397
[10] A. Yang and S. Yu, “Research on teaching evaluation system based on machine learning,” Mobile Information Systems, vol. 2022, pp. 1–10, 02 2022.
[11] M. Husain and S. Khan, “Students’ feedback: An effective tool in teachers’ evaluation system,” International Journal of Applied and Basic Medical
Research, vol. 6, p. 178, 07 2016.
[12] S. Falcon and J. León, “Towards an optimised evaluation of teachers’ discourse: The case of engaging messages,” 12 2024.
[13] F.-C. Lin, H.-H. Ngo, C.-R. Dow, K.-H. Lam, and L. Hung Linh, “Student behavior recognition system for the classroom environment based on skeleton
pose estimation and person detection,” Sensors, vol. 21, p. 5314, 08 2021.
[14] P. Afsar, P. Cortez, and H. Santos, “Automatic visual detection of human behavior: A review from 2000 to 2014,” Expert Systems with Applications,
vol. 42, no. 20, pp. 6935–6956, 2015. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S0957417415003516
[15] W. Shuo, C. Zengzhao, W. Mengke, S. Yawen, and Z. Shenghu, “Teacher attention measurement based on head pose estimation,” in 2022 International
Conference on Intelligent Education and Intelligent Research (IEIR), 2022, pp. 1–7.
[16] B. Priya, N. J.M, and G. Thangavel, An Analysis of the Applications of Natural Language Processing in Various Sectors, 10 2021.
[17] S. C. Fanni, M. Febi, G. Aghakhanyan, and E. Neri, “Natural language processing,” in Introduction to artificial intelligence. Springer, 2023, pp. 87–99.
[18] A. Rajput, “Natural language processing, sentiment analysis, and clinical analytics,” in Innovation in health informatics. Elsevier, 2020, pp. 79–97.
[19] Z. Kastrati, F. Dalipi, A. S. Imran, K. Pireva Nuci, and M. A. Wani, “Sentiment analysis of students’ feedback with nlp and deep learning: A systematic
mapping study,” Applied Sciences, vol. 11, no. 9, p. 3986, 2021.
[20] J. R. Jim, M. A. R. Talukder, P. Malakar, M. M. Kabir, K. Nur, and M. F. Mridha, “Recent advancements and challenges of nlp-based sentiment analysis:
A state-of-the-art review,” Natural Language Processing Journal, p. 100059, 2024.
[21] F. M. Plaza-del Arco, A. Curry, A. C. Curry, and D. Hovy, “Emotion analysis in nlp: Trends, gaps and roadmap for future directions,” arXiv preprint
arXiv:2403.01222, 2024.
[22] M. Karabacak, A. J. Schupper, M. T. Carr, Z. L. Hickman, and K. Margetis, “From text to insight: a natural language processing-based analysis of topics
and trends in neurosurgery,” Neurosurgery, vol. 94, no. 4, pp. 679–689, 2024.

