This thesis introduced a novel approach to evaluate pedagogical outcomes using multimodal data, unifying visual, audio, and linguistic evidence into a single analytic framework. The overall system was designed for practical classroom conditions: robust preprocessing, modular feature extraction for each modality, and a late-fusion strategy that preserves interpretability while leveraging complementarities among signals. On top of this representation, we trained supervised models to predict session-level verdicts of teaching effectiveness.

The empirical results provide a clear narrative. The fused multimodal representation consistently outperformed single-modality baselines across accuracy, F1-score, ROC-AUC, and cross-validation stability. With Logistic Regression, the combined features achieved 71.33% accuracy, a 4.33 percentage point improvement over the best individual stream (linguistic at 67.0%), while also reducing performance variance across folds (mean cross-validation around 70.4% with lower standard deviation). Support Vector Machines exhibited a similar pattern, reinforcing the conclusion that visual movement cues, vocal prosody, and discourse structure are complementary in characterizing effective pedagogy.

Beyond headline numbers, the findings demonstrate that integrated analysis captures aspects of classroom practice that no single modality can fully represent. Visual features contextualize presence and movement; audio features reflect prosodic emphasis and pacing; linguistic features reveal discourse clarity and structure. Their combination yields a more balanced decision profile, reducing false negatives and enabling more actionable insights for instructional improvement. Taken together, these results validate the central claim of this work: multimodal integration provides a more reliable and informative account of pedagogical effectiveness than unimodal analysis.

\newpage
\subsection{Limitations}
While promising, the present study is bounded by several practical constraints that shape how the results should be interpreted. First, although our conceptual taxonomy spans six pedagogical categories, the current experimental setting is binary. We therefore employed binary classifiers—Logistic Regression and SVM—which are well suited to two-class problems and supported the stabilization of the pipeline. This design choice simplifies the task but inevitably limits granularity, compressing a nuanced space of teaching quality into a dichotomy. Second, the dataset is moderate in size and institutional diversity. Generalization to different disciplines, instructional styles, and classroom configurations remains a critical next step. Third, our feature design and late-fusion strategy favor interpretability and robustness but may under-express rich temporal dependencies and cross-modal interactions that unfold throughout a class session. Finally, broader external validity, fairness, and privacy considerations—such as subgroup performance consistency, calibration, and privacy-preserving operation—require deeper, systematic evaluation prior to large-scale deployment.

\subsection{Future Work}
\subsubsection{From Binary to Multiclass Evaluation}
The immediate priority is to move from a binary verdict to the full six-class taxonomy envisioned by the system. In this thesis we used two classes, which allowed us to rely on Logistic Regression and SVM as principled baselines. Future iterations will adopt \emph{multiclass} learning: multinomial Logistic Regression, one-vs-rest/one-vs-one SVM, tree ensembles, and deep architectures that natively handle multiple categories. Realizing this shift requires a carefully constructed multiclass dataset with explicit labeling rubrics, adjudication guidelines, and inter-rater reliability studies to ensure consistency.

\subsubsection{Scaling Data and Labeling Efficiency}
To strengthen generalization, the dataset should expand across institutions, subjects, delivery formats, and room configurations. Alongside scale, efficient supervision strategies—active learning, weak supervision, semi-supervised and self-supervised pretraining—can reduce annotation cost while improving coverage in underrepresented conditions. These strategies are particularly relevant for long-form classroom data, where exhaustive labeling is expensive.

\subsubsection{Temporal and Multimodal Representation Learning}
Richer temporal modeling is a natural evolution. Sequence models and temporal attention can capture discourse flow, gesture timing, and prosodic evolution, while self-supervised representation learning (e.g., Wav2Vec2 for audio, VideoMAE/TimeSformer for video, and BERT-family models for text) can provide stronger foundations when labeled data is limited. Beyond late fusion, hybrid attention-based fusion and multimodal transformers may better align cross-modal cues and handle missing streams gracefully.

\subsubsection{Reliability, Fairness, and Privacy}
For trustworthy deployment, calibrated probabilities, uncertainty estimation, and cost-sensitive learning should be incorporated to reflect asymmetric risks in evaluation outcomes. Systematic audits of subgroup performance across demographics, disciplines, and class sizes are necessary to monitor potential biases and ensure equitable behavior. Privacy-preserving operation—on-device or edge processing where feasible, consent management, de-identification such as face blurring, and exploration of federated learning or differential privacy—remains a core requirement.

\subsubsection{Toward Real-World Use}
Robustness to noise, occlusion, device variability, and domain shift must be stress-tested, with domain adaptation and continual learning supporting long-term reliability. From a product perspective, low-latency inference, model compression, and streaming operation are essential for practical classroom integration. Equally important are educator-facing summaries and dashboards that translate metrics—such as classroom coverage, pause ratio, or discourse structure—into concrete, actionable recommendations.

\subsection{Closing Remarks}
This thesis demonstrates that multimodal integration is a viable and effective pathway for automated pedagogical assessment. Advancing toward a validated, fair, privacy-aware, and multiclass-capable system—grounded in larger and more diverse datasets, temporal modeling, and modern multimodal learning—represents the next phase. With these developments, the approach presented here can mature into a scalable, institution-ready framework that supports meaningful, actionable teacher feedback and continuous instructional improvement.
